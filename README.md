# Natural Language Procession Specialization

### My Scores
- [Course 1: 100%](C1-natural-language-processing-with-classification-and-vector-spaces/grade.png)
- [Course 2: 100%](C2-natural-language-processing-with-probabilistic-models/grade.png)
- [Course 3: 100%](C3-natural-language-processing-with-sequence-models/grade.png)
- [Course 4: 97.50%](C4-natural-language-processing-with-attention-models/grade.png)

## Course Structure
Each course is spread out in weeks, and are made up of video slides, lab sessions, quizzes, assignments, related course materials, code and data

## Course 1: Natural Language Processing with Classification and Vector Spaces

### Week 1
#### Slides
- [Supervised ML and Sentiment Analysis](C1-natural-language-processing-with-classification-and-vector-spaces/Week1/C1_W1.pdf)
#### Labs
- [Preprocessing](C1-natural-language-processing-with-classification-and-vector-spaces/Week1/C1_W1_lecture_nb_01_preprocessing.ipynb)
- [Building and Visualizing word frequencies](C1-natural-language-processing-with-classification-and-vector-spaces/Week1/C1_W1_lecture_nb_02_word%20frequencies.ipynb)
- [Visualizing tweets and the Logistic Regression model](C1-natural-language-processing-with-classification-and-vector-spaces/Week1/C1_W1_lecture_nb_03_logistic_regression_model.ipynb)
#### Assignments
- [Assignment 1: Logistic Regression](C1-natural-language-processing-with-classification-and-vector-spaces/Week1/C1_W1_Assignment.ipynb)

### Week 2
#### Slides
- [Probability and Bayesâ€™ Rule](C1-Introduction-to-machine-learning-in-production/Week2/C1_W2.pdf)
#### Labs
- [Visualizing Naive Bayes](C1-natural-language-processing-with-classification-and-vector-spaces/Week2/C1_W2_lecture_nb_01_visualizing_naive_bayes.ipynb)
#### Assignments
- [Assignment 2: Naive Bayes](C1-natural-language-processing-with-classification-and-vector-spaces/Week2/C1_W2_Assignment.ipynb)

### Week 3
#### Slides
- [Vector Space Models](C1-natural-language-processing-with-classification-and-vector-spaces/Week3/C1_W3.pdf)
#### Labs
- [Linear algebra in Python with NumPy](C1-natural-language-processing-with-classification-and-vector-spaces/Week3/C1_W3_lecture_nb_01_linear_algebra.ipynb)
- [Manipulating word embeddings ](C1-natural-language-processing-with-classification-and-vector-spaces/Week3/C1_W3_lecture_nb_02_manipulating_word_embeddings.ipynb)
- [Another explanation about PCA](C1-natural-language-processing-with-classification-and-vector-spaces/Week3/C1_W3_lecture_nb_03_pca.ipynb)
#### Assignments
- [Assignment 3: Hello Vectors](C1-natural-language-processing-with-classification-and-vector-spaces/Week3/C1_W3_Assignment.ipynb)

### Week 4
#### Slides
- [Transforming word vectors](C1-natural-language-processing-with-classification-and-vector-spaces/Week4/C1_W4.pdf)
#### Labs
- [Vector manipulation in Python](C1-natural-language-processing-with-classification-and-vector-spaces/Week4/C1_W4_lecture_nb_01_vector_manipulation.ipynb)
- [Hash functions and multiplanes](C1-natural-language-processing-with-classification-and-vector-spaces/Week4/C1_W4_lecture_nb_02_hash_functions_and_multiplanes.ipynb)
#### Assignments
- [Assignment 4 - Naive Machine Translation and LSH](C1-natural-language-processing-with-classification-and-vector-spaces/Week4/C1_W4_Assignment.ipynb)

## Course 2: Natural Language Processing with Probabilistic Models

### Week 1
#### Slides
- [Autocorrect](C2-natural-language-processing-with-probabilistic-models/Week1/C2_W1.pdf)
#### Labs
- [NLP Course 2 Week 1 Lesson : Building The Model - Lecture Exercise 01](C2-natural-language-processing-with-probabilistic-models/Week1/C2_W1_lecture_nb_01_building_the_vocabulary_model.ipynb)
- [NLP Course 2 Week 1 Lesson : Building The Model - Lecture Exercise 02](C2-natural-language-processing-with-probabilistic-models/Week1/C2_W1_lecture_nb_02_candidates_from_edits.ipynb)
#### Assignments
- [Assignment 1: Autocorrect](C2-natural-language-processing-with-probabilistic-models/Week1/C2_W1_Assignment.ipynb)

### Week 2
#### Slides
- [Part of Speech Tagging](C2-natural-language-processing-with-probabilistic-models/Week2/C2_W2.pdf)
#### Labs
- [Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words](C2-natural-language-processing-with-probabilistic-models/Week2/C2_W2_lecture_nb_1_strings_tags.ipynb)
- [Parts-of-Speech Tagging - Working with tags and Numpy](C2-natural-language-processing-with-probabilistic-models/Week2/C2_W2_lecture_nb_2_numpy.ipynb)
#### Assignments
- [Assignment 2: Parts-of-Speech Tagging (POS)](C2-natural-language-processing-with-probabilistic-models/Week2/C2_W2_Assignment.ipynb)

### Week 3
#### Slides
- [N-grams and Probabilities](C2-natural-language-processing-with-probabilistic-models/Week3/C2_W3.pdf)
#### Labs
- [N-grams Corpus preprocessing](C2-natural-language-processing-with-probabilistic-models/Week3/C2_W3_lecture_nb_01_corpus_preprocessing.ipynb)
- [Building the language model](C2-natural-language-processing-with-probabilistic-models/Week3/C2_W3_lecture_nb_02_building_the_language_model.ipynb)
- [Out of vocabulary words (OOV)](C2-natural-language-processing-with-probabilistic-models/Week3/C2_W3_lecture_nb_03_oov.ipynb)
#### Assignments
- [Assignment 3: Language Models: Auto-Complete](C2-natural-language-processing-with-probabilistic-models/Week3/C2_W3_Assignment.ipynb)

### Week 4
#### Slides
- [Basic Word Representations](C2-natural-language-processing-with-probabilistic-models/Week4/C2_W4.pdf)
#### Labs
- [Word Embeddings First Steps: Data Preparation](C2-natural-language-processing-with-probabilistic-models/Week4/C2_W4_lecture_nb_1_data_prep.ipynb)
- [Word Embeddings: Intro to CBOW model, activation functions and working with Numpy](C2-natural-language-processing-with-probabilistic-models/Week4/C2_W4_lecture_nb_2_intro_to_CBOW.ipynb)
- [Word Embeddings: Training the CBOW model](C2-natural-language-processing-with-probabilistic-models/Week4/C2_W4_lecture_nb_3_training_the_CBOW.ipynb)
- [Word Embeddings: Hands On](C2-natural-language-processing-with-probabilistic-models/Week4/C2_W4_lecture_nb_4_word_embeddings_hands_on.ipynb)
- [Word Embeddings: Ungraded Practice Notebook](C2-natural-language-processing-with-probabilistic-models/Week4/C2_W4_lecture_nb_5_word_embeddings_step_by_step.ipynb)
#### Assignments
- [Assignment 4: Word Embeddings](C2-natural-language-processing-with-probabilistic-models/Week4/C2_W4_Assignment.ipynb)

## Course 3: Natural Language Processing with Sequence Models

### Week 1
#### Slides
- [Neural Networks for Sentiment Analysis](C3-natural-language-processing-with-sequence-models/Week1/C3_W1.pdf)
#### Labs
- [Trax : Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week1/NLP_C3_W1_lecture_nb_01_trax_intro.ipynb)
- [Classes and subclasses ](C3-natural-language-processing-with-sequence-models/Week1/NLP_C3_W1_lecture_nb_02_classes.ipynb)
- [Data generators](C3-natural-language-processing-with-sequence-models/Week1/NLP_C3_W1_lecture_nb_03_data_generators.ipynb)
#### Assignments
- [Assignment 1: Sentiment with Deep Neural Networks](C3-natural-language-processing-with-sequence-models/Week1/C3_W1_Assignment.ipynb)

### Week 2
#### Slides
- [Traditional Language models](C3-natural-language-processing-with-sequence-models/Week2/C3_W2.pdf)
#### Labs
- [Hidden State Activation: Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week2/C3_W2_lecture_nb_1_Hidden_State_Activation.ipynb)
- [anilla RNNs, GRUs and the `scan` function](C3-natural-language-processing-with-sequence-models/Week2/C3_W2_lecture_nb_2_RNNs.ipynb)
- [Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week2/C3_W2_lecture_nb_3_perplexity.ipynb)
- [Creating a GRU model using Trax: Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week2/C3_W2_lecture_nb_4_GRU.ipynb)

### Week 3
#### Slides
- [RNNs andVanishing Gradients](C3-natural-language-processing-with-sequence-models/Week3/C3_W3.pdf)
#### Labs
- [Vanishing Gradients and Exploding Gradients in RNNs: Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week3/C3_W3_Lecture_Notebook_Vanishing_Gradients.ipynb)

### Week 4
#### Slides
- [Siamese Networks](C3-natural-language-processing-with-sequence-models/Week4/C3_W4.pdf)
#### Labs
- [Creating a Siamese model using Trax: Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week4/C3_W4_lecture_nb_1_siamese.ipynb)
- [Modified Triplet Loss: Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week4/C3_W4_lecture_nb_2_Modified_Triplet_Loss.ipynb)
- [Evaluate a Siamese model: Ungraded Lecture Notebook](C3-natural-language-processing-with-sequence-models/Week4/C3_W4_lecture_nb_3_accuracy.ipynb)


## Course 4: Natural Language Processing with Attention Models

### Week 1
#### Slides
- [Seq2Seq model for NMT](C4-natural-language-processing-with-attention-models/Week1/C4_W1.pdf)
#### Labs
- [Basic Attention Operation: Ungraded Lab](C4-natural-language-processing-with-attention-models/Week1/C4_W1_Ungraded_Lab_1_Basic_Attention.ipynb)
- [Scaled Dot-Product Attention: Ungraded Lab](C4-natural-language-processing-with-attention-models/Week1/C4_W1_Ungraded_Lab_2_QKV_Attention.ipynb)
- [Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab](C4-natural-language-processing-with-attention-models/Week1/C4_W1_Ungraded_Lab_3_Bleu_Score.ipynb)
- [Stack Semantics in Trax: Ungraded Lab](C4-natural-language-processing-with-attention-models/Week1/C4_W1_Ungraded_Lab_4_Stack_Semantics.ipynb)

### Week 2
#### Slides
- [Transformers vs RNNs](C4-natural-language-processing-with-attention-models/Week2/C4_W2.pdf)
#### Labs
- [The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook](C4-natural-language-processing-with-attention-models/Week2/C4_W2_Ungraded_Lab_1_Attention.ipynb)
- [The Transformer Decoder: Ungraded Lab Notebook](C4-natural-language-processing-with-attention-models/Week2/C4_W2_Ungraded_Lab_2_Transformer_Decoder.ipynb)

### Week 3
#### Slides
- [Question Answering & Transfer Learning](C4-natural-language-processing-with-attention-models/Week3/C4_W3.pdf)
#### Labs
- [SentencePiece and BPE ](C4-natural-language-processing-with-attention-models/Week3/C4_W3_SentencePiece_and_BPE.ipynb)

### Week 4
#### Slides
- [Tasks with Long Sequences](C4-natural-language-processing-with-attention-models/Week4/C4_W4.pdf)
#### Labs
- [Reformer Efficient Attention: Ungraded Lab](C4-natural-language-processing-with-attention-models/Week4/C4_W4_Ungraded_Lab_1_Reformer_LSH.ipynb)
- [Putting the "Re" in Reformer: Ungraded Lab](C4-natural-language-processing-with-attention-models/Week4/C4_W4_Ungraded_Lab_2_Revnet.ipynb)

## Disclaimer
The solutions presented are intended to serve as reference for other learners who enroll in this course.